# activation-function-analysis
This project compares various activation functions (ReLU, Sigmoid, Tanh, Leaky ReLU) in deep learning models, analyzing their impact on training dynamics such as loss and accuracy. It includes code for experiments, visualizations of results, and a tutorial to guide users in modifying and running their own analyses.
